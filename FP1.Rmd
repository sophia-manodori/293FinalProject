---
title: "FP1"
output: html_document
---

```{r}
# load packages
library(tidyverse)
library(MASS)
library(e1071)
```

```{r}
# load data
full_data <- read.csv("large_dataset.csv")
```

```{r}
# create column for row number
full_data$ID <- 1:nrow(full_data)

# select for winner and for fighter statistics differences
# differences: Red minus Blue
log_data <- full_data %>%
  dplyr::select(ID, winner, wins_total_diff, losses_total_diff, age_diff, height_diff, weight_diff, reach_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_acc_total_diff, str_def_total_diff, td_def_total_diff, sub_avg_diff, td_avg_diff)
```

```{r}
# Logistic regression
log_data$winner = as.factor(log_data$winner)
glm.fit <- glm(winner ~ ., family = binomial, data = log_data)
summary(glm.fit)
```
The logistic regression model shows that the significant predictors of who wins are: wins_total_diff, losses_total_diff, age_diff, weight_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_def_total_diff, and td_avg_diff. All of those variables have a p value below 0.001. We will use these variables for LDA, QDA, and Naive Bayes models.

```{r}
data <- log_data %>%
  dplyr::select(ID, winner, wins_total_diff, losses_total_diff, age_diff, weight_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_def_total_diff, td_avg_diff)
```

```{r}
# select first 80% of the data to be the training set
train1 <- (data$ID <= round(0.8*nrow(data), 0))
test1 <- data[!train1, ]
winner.test1 <- data$winner[!train1]
nrow(test1)
```

```{r, warning=FALSE}
# LDA
lda.fit <- lda(winner ~ ., data = data, subset = train1)
lda.pred <- predict(lda.fit, test1)
lda.class <- lda.pred$class
table(lda.class, winner.test1)

57+264+35+923 # number of observations in test set used for analysis

(57+35)/(57+264+35+923) # true Blue proportion in test
(264+923)/(57+264+35+923) # true Red proportion in test

(264+35)/(57+264+35+923) # overall error rate
35/(57+35) # Blue class error rate
264/(264+923) # Red class error rate
```
The overall test error rate is 23%. The Blue class has an error rate of 38% and the Red class has an error rate of 22%.

```{r, warning=FALSE}
# QDA 
qda.fit <- qda(winner ~ . , data = data, subset = train1)
qda.fit
qda.class <- predict(qda.fit, test1)$class
table(qda.class, winner.test1)

52+40+307+880 # number of observations in test set used for analysis

(40+307)/(52+40+307+880) # overall error rate
40/(52+40) # Blue error rate
307/(307+880) # Red error rate
```
42% of the probabilities are Blue, 58% of the probabilities are Red. The overall error rate is 27%. The error rate for the Blue class is 43% and the error rate for the Red class is 25%. Both error rates are worse than the LDA model.

```{r}
# Naive Bayes
nb.fit <- naiveBayes(winner ~ ., data = data, subset = train1)
nb.class <- predict(nb.fit, test1)
table(nb.class, winner.test1)

51+41+302+1094 # number of observations in test set used for analysis

(41+302)/(51+41+302+1094) # overall error rate
41/(51+41) # Blue error rate
302/(302+1094) # Red error rate
```
The overall error rate is 23%. The error rate for the Blue class is 44% and the error rate for the Red class is 22%. Although the overall error rate is the same as for the LDA model, the error rate for the Blue class is 6 percentage points better in the LDA model, meaning the LDA model performs the best with this dataset. All models do substantially better at predicting the Red winner than at predicting the Blue winner. This is likely due to the majority of the wins being Red winners, as the Red side is usually the champion's side, so the greater number of Red wins allows the model to understand and predict Red wins better.

