---
title: "FP1"
output: html_document
---

```{r}
# load packages
library(tidyverse)
library(MASS)
library(e1071)
library(biotools)
library(car)
```

```{r}
# load data
full_data <- read.csv("large_dataset.csv")
```

```{r}
# create column for row number
full_data$ID <- 1:nrow(full_data)

# select for winner and for fighter statistics differences
# differences: Red minus Blue
log_data <- full_data %>%
  dplyr::select(ID, winner, wins_total_diff, losses_total_diff, age_diff, height_diff, weight_diff, reach_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_acc_total_diff, str_def_total_diff, td_def_total_diff, sub_avg_diff, td_avg_diff) %>%
  drop_na()

log_data$winner = as.factor(log_data$winner)

# select first 80% of the data to be the training set
log_train1 <- (log_data$ID <= round(0.8*nrow(log_data), 0))
log_test1 <- log_data[!log_train1, ]
winner.log_test1 <- log_data$winner[!log_train1]
nrow(log_test1)
```

```{r}
# Logistic regression
glm.fit <- glm(winner ~ ., family = binomial, data = log_data, subset = log_train1)

summary(glm.fit)
```
The logistic regression model shows that the significant predictors of who wins are: wins_total_diff, losses_total_diff, age_diff, weight_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_def_total_diff, and td_avg_diff. All of those variables have a p value below 0.001. We will test this logistic regression model as a classification variable and we will use these significant variables for LDA, QDA, and Naive Bayes classification models and compare all models between each other to see which performs best as a classification method.

```{r}
glm.probs <- predict(glm.fit, log_test1, type = "response")
contrasts(log_data$winner) # Red = 1
glm.pred <- rep("Blue", 1537)
glm.pred[glm.probs > .5] = "Red"
table(glm.pred, winner.log_test1)

200+175+326+836 # number of observations in test set used for analysis
1-mean(glm.pred == winner.log_test1) # overall error rate
(200+836)/(200+175+326+836) # overall sucess rate
175/(200+175) # Blue class error rate
326/(326+836)
```
With all predictors, the logistic model has an overall error rate of 33%. The error rate for the Blue class is 47%, while the error rate for the Red class is significantly lower, at 28%.

```{r}
data <- full_data %>%
  dplyr::select(ID, winner, wins_total_diff, losses_total_diff, age_diff, weight_diff, SLpM_total_diff, SApM_total_diff, sig_str_acc_total_diff, td_def_total_diff, td_avg_diff) %>%
  drop_na()

data$winner = as.factor(data$winner)
```

```{r}
# select first 80% of the data to be the training set
 train <- (data$ID <= round(0.8*nrow(data), 0))
 test <- data[!train, ]
 write.csv(data[c(1,2,3),], "slay.csv", row.names = FALSE)
 
 winner.test <- data$winner[!train]
 nrow(test)
```

```{r}
glm.fit <- glm(winner ~ .-ID, family = binomial, data = data, subset = train)
glm.probs <- predict(glm.fit, test, type = "response")
contrasts(data$winner) # Red = 1
glm.pred <- rep("Blue", 1446)
glm.pred[glm.probs > .5] = "Red"
table(glm.pred, winner.test)

(59+315)/1446 # overall error rate
59/(96+59) # blue error rate
315/(315+976) # red error rate
```
The logistic model using only the significant predictors has an overall error rate of 26%. The error rate for the Blue class is 38%, while the error rate for the Red class is significantly lower, at 24%.

```{r}
boxM_result <- boxM(data[ , -which(names(data) == "winner")], data$winner)
boxM_result$p.value
```

Because Box's M test gave us a p value well below 0.05, there is extremely strong statistical evidence that the covariance matrices are not equal between the Red class and the Blue class. This violates an assumption of LDA, so we can expect logistic regression to perform somewhat better than LDA. 

```{r}
# LDA
lda.fit <- lda(winner ~ .-ID, data = data, subset = train)
lda.pred <- predict(lda.fit, test)
lda.class <- lda.pred$class
table(lda.class, winner.test)

(60+312)/(95+312+60+979) # overall error rate
60/(60+95) # blue error rate
312/(312+979) # red error rate
```
The overall test error rate for the LDA model is 26%. The Blue class has an error rate of 39% and the Red class has an error rate of 24%.

```{r, warning=FALSE}
# QDA 
qda.fit <- qda(winner ~ .-ID, data = data, subset = train)
qda.class <- predict(qda.fit, test)$class
table(qda.class, winner.test)

(62+338)/(93+62+338+953) # overall error rate 
62/(93+62) # Blue error rate 
338/(338+953) # Red error rate 
```
The overall error rate for the QDA model is 28%. The error rate for the Blue class is 40% and the error rate for the Red class is 26%.

```{r}
cor(data[, sapply(data, is.numeric)])
```
The correlation matrix shows no high correlations between pairs of predictors except between the variables for wins and losses. This correlation may affect the performance of Naive Bayes.

```{r}
# Naive Bayes
nb.fit <- naiveBayes(winner ~ .-ID, data = data, subset = train)
nb.class <- predict(nb.fit, test)
table(nb.class, winner.test)

(63+363)/(92+63+363+928) # overall error rate
63/(63+92) # Blue error rate
363/(363+928) # Red error rate
```
The overall error rate for Naive Bayes is 29%. The error rate for the Red class is 28% and the error rate for the Blue class is 41%. 

By comparing error rates, we conclude that although logistic regression and LDA have the same overall error rate and error rate for the Red class, logistic regression performs better by 1 percentage point for the Blue class. All models do substantially better at predicting the Red winner than at predicting the Blue winner. This is likely due to the majority of the wins being Red winners, as the Red side is usually the champion's side, so the greater number of Red wins allows the model to understand and predict Red wins better. We will use the LDA model to predict the outcome for the fights on Saturday April 26th. 
